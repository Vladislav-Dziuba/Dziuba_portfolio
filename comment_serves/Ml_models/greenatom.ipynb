{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af791a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Владислав\n",
      "[nltk_data]     Дзюба\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Владислав\n",
      "[nltk_data]     Дзюба\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Владислав\n",
      "[nltk_data]     Дзюба\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nltk\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from tensorflow.keras.layers import Dense, LSTM, Input, Dropout, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5e4eaa",
   "metadata": {},
   "source": [
    "### Реализуем алгоритм логистической регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "083d64ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### функция для предобработки текстов\n",
    "def preprocess(link,status):\n",
    "    ## открываем папку по названию link\n",
    "    content = os.listdir(link)\n",
    "    data_list = []\n",
    "    \n",
    "    ### циклом добавляем содержимое папки в список, предварительно очищая от мусорных символов\n",
    "    for i in content:\n",
    "        with open(f'{link}/{i}','r',encoding='utf-8') as t:\n",
    "            data = t.read().lower()\n",
    "            data = re.sub('[^a-z ]','', data)\n",
    "            data_list.append(data)\n",
    "    \n",
    "    ## создаем датасет\n",
    "    df = pd.DataFrame({'Comments':data_list,'ID':content})\n",
    "    ### присваеваем статус комментариям\n",
    "    df['Rating']=status\n",
    "    df['evaluation'] = df['ID'].apply(lambda x: x[-6:-4]).apply(lambda y: y.replace('_','') if '_' in y else y)\n",
    "    \n",
    "    \n",
    "    ### выкидываем стоп-слова    \n",
    "    df['Comments'] = df['Comments'].apply(lambda x: [i for i in x.split(' ') if i not in stopwords_set])\n",
    "    \n",
    "    ### лемматизируем слова отзывов\n",
    "    df['Comments'] = df['Comments'].apply(lambda b: [wordnet_lemmatizer.lemmatize(word) for word in b])\n",
    "    \n",
    "    ### обьединяем каждый отзывыв из списка обратно в строку\n",
    "    df['Comments'] = df['Comments'].apply(lambda s: ' '.join(s))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aed90f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### обработаем все 4 датасета: тестовые и тренировочные\n",
    "df_neg_train = preprocess('neg_train',0)\n",
    "df_pos_train = preprocess('pos_train',1)\n",
    "df_neg_test = preprocess('neg_test',0)\n",
    "df_pos_test = preprocess('pos_test',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeaf5660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>ID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>evaluation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first saw ad like oh go he done high school mu...</td>\n",
       "      <td>6449_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>girl folly sort halfcomedy halfmockumentary lo...</td>\n",
       "      <td>7235_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>started watching show first season beginning p...</td>\n",
       "      <td>1017_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>interesting usual porn movie fantasy adventure...</td>\n",
       "      <td>9954_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suppose  film supposed  cool   looking back   ...</td>\n",
       "      <td>11791_2.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>poor film certainly belongs make feature film ...</td>\n",
       "      <td>3452_2.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>saw movie new later rented japan three year af...</td>\n",
       "      <td>8491_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>meandering tale mob revenge simply interesting...</td>\n",
       "      <td>954_4.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>first didnt like acting really hamlet standard...</td>\n",
       "      <td>2480_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spectacle hard fault nihon chinbotsu japanese ...</td>\n",
       "      <td>9513_9.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>oscar winner karl malden sally field shirley ...</td>\n",
       "      <td>3797_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>horror film curious thing sometimes manage stu...</td>\n",
       "      <td>11076_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>start slowly showing dreary life two housewife...</td>\n",
       "      <td>431_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>year losing gorgeous jane parker maureen osull...</td>\n",
       "      <td>11273_9.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>nobody know anybody conspiracy theory thriller...</td>\n",
       "      <td>4124_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>attack fifty foot woman alison hayes opened do...</td>\n",
       "      <td>2604_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>think everyone quite disappointed scifi flick ...</td>\n",
       "      <td>1240_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>best fan boy movie ive ever watched save free ...</td>\n",
       "      <td>7124_10.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>thank god didnt waste money renting downloaded...</td>\n",
       "      <td>10099_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>add paulie parrot beloved movie animal charact...</td>\n",
       "      <td>9681_9.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comments           ID  Rating  \\\n",
       "0   first saw ad like oh go he done high school mu...   6449_8.txt       1   \n",
       "1   girl folly sort halfcomedy halfmockumentary lo...   7235_8.txt       1   \n",
       "2   started watching show first season beginning p...   1017_8.txt       1   \n",
       "3   interesting usual porn movie fantasy adventure...   9954_8.txt       1   \n",
       "4   suppose  film supposed  cool   looking back   ...  11791_2.txt       0   \n",
       "5   poor film certainly belongs make feature film ...   3452_2.txt       0   \n",
       "6   saw movie new later rented japan three year af...   8491_7.txt       1   \n",
       "7   meandering tale mob revenge simply interesting...    954_4.txt       0   \n",
       "8   first didnt like acting really hamlet standard...   2480_8.txt       1   \n",
       "9   spectacle hard fault nihon chinbotsu japanese ...   9513_9.txt       1   \n",
       "10   oscar winner karl malden sally field shirley ...   3797_1.txt       0   \n",
       "11  horror film curious thing sometimes manage stu...  11076_1.txt       0   \n",
       "12  start slowly showing dreary life two housewife...    431_8.txt       1   \n",
       "13  year losing gorgeous jane parker maureen osull...  11273_9.txt       1   \n",
       "14  nobody know anybody conspiracy theory thriller...   4124_1.txt       0   \n",
       "15  attack fifty foot woman alison hayes opened do...   2604_7.txt       1   \n",
       "16  think everyone quite disappointed scifi flick ...   1240_1.txt       0   \n",
       "17  best fan boy movie ive ever watched save free ...  7124_10.txt       1   \n",
       "18  thank god didnt waste money renting downloaded...  10099_1.txt       0   \n",
       "19  add paulie parrot beloved movie animal charact...   9681_9.txt       1   \n",
       "\n",
       "   evaluation  \n",
       "0           8  \n",
       "1           8  \n",
       "2           8  \n",
       "3           8  \n",
       "4           2  \n",
       "5           2  \n",
       "6           7  \n",
       "7           4  \n",
       "8           8  \n",
       "9           9  \n",
       "10          1  \n",
       "11          1  \n",
       "12          8  \n",
       "13          9  \n",
       "14          1  \n",
       "15          7  \n",
       "16          1  \n",
       "17         10  \n",
       "18          1  \n",
       "19          9  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### обьединим все в один дата сэт\n",
    "new_df = pd.concat([df_pos_train, df_neg_train,df_pos_test,df_neg_test],ignore_index=True)\n",
    "new_df = new_df.sample(frac=1,random_state = 42).reset_index(drop=True)\n",
    "new_df.drop_duplicates()\n",
    "new_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2e5ab86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50000x164761 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4877267 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### обучим вектораейзер на корпусе всех слов со всех отзывов\n",
    "data_corp = [ \" \".join(new_df[new_df['Rating'] == l]['Comments'].tolist()) for l in list(new_df.Rating.unique()) ]\n",
    "vectorizer = TfidfVectorizer()#ngram_range=(1,2)\n",
    "vectorizer.fit(data_corp)\n",
    "\n",
    "res_tfidf = vectorizer.transform(new_df['Comments'].tolist())\n",
    "res_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfeef211",
   "metadata": {},
   "outputs": [],
   "source": [
    "### стандартная процедура\n",
    "X_tr, X_ts, y_tr, y_ts=train_test_split(res_tfidf, new_df['Rating'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc32f553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.9086444251519286\n",
      "test: 0.8826870348609479\n"
     ]
    }
   ],
   "source": [
    "### обучаем модель и смотрим результат\n",
    "lr = LogisticRegression(solver='liblinear',penalty='l2').fit(X_tr, y_tr)\n",
    "\n",
    "y_pred_test = lr.predict(X_ts)\n",
    "y_pred_train = lr.predict(X_tr)\n",
    "\n",
    "\n",
    "\n",
    "print('train:',f1_score(y_tr, y_pred_train))\n",
    "print('test:',f1_score(y_ts, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e9f39d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### сохраняем модель\n",
    "rom joblib import dump, load\n",
    "\n",
    "dump(lr, 'vectorizer_model.joblib') \n",
    "dump(vectorizer, 'vectorizer_model.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f49374",
   "metadata": {},
   "source": [
    "### Реализуем такой же сентимент-анализ на LSTM сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf24ea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "### пределим сколько у нас положительных и отрицательных отзывов\n",
    "count_pos = len(df_pos_train)+len(df_pos_test)\n",
    "count_neg = len(df_neg_train)+len(df_neg_test)\n",
    "\n",
    "total_count = count_pos+count_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "896d2da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### зададим параметры для токенизации, 10000 слов кажется оптимальным, но не самым лучшим\n",
    "maxWordsCount = 10000\n",
    "tokenizer = Tokenizer(num_words=maxWordsCount, filters='!–\"—#$%&amp;()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r«»', lower=True, split=' ', char_level=False)\n",
    "tokenizer.fit_on_texts(new_df['Comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0242961d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('first', 17202), ('s', 120731), ('w', 115108), ('d', 91861), ('like', 40556), ('oh', 3047), ('go', 17412), ('he', 23228), ('done', 5856), ('high', 3954)]\n",
      "first saw ad like oh go he done high school musical cant coast along he making appearance disney sho\n"
     ]
    }
   ],
   "source": [
    "dist = list(tokenizer.word_counts.items())\n",
    "print(dist[:10])\n",
    "print(new_df['Comments'][0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87dfff5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...  177   34   72]\n",
      " [   0    0    0 ... 5598 1186    8]\n",
      " [   0    0    0 ...  103  111   34]\n",
      " ...\n",
      " [  58   45    7 ...  279   57 1031]\n",
      " [   0    0    0 ...  247    6  882]\n",
      " [3305   12 4969 ...    7   64 2156]]\n"
     ]
    }
   ],
   "source": [
    "#### определим длинну каждого текста, под которую они будут подгонятся нулями или обрезаться\n",
    "max_text_len = 150\n",
    "data = tokenizer.texts_to_sequences(new_df['Comments'])\n",
    "data_pad = pad_sequences(data, maxlen=max_text_len)\n",
    "print(data_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2e47142",
   "metadata": {},
   "outputs": [],
   "source": [
    "### создадим целевую переменную в нужно для подачи в сеть формате\n",
    "prep_y = []\n",
    "for i in new_df['Rating']:\n",
    "    if i == 1:\n",
    "        prep_y.append([1,0])\n",
    "    else:\n",
    "        prep_y.append([0,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee67c2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 150) (50000, 2)\n"
     ]
    }
   ],
   "source": [
    "X = data_pad\n",
    "Y = np.array(prep_y)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6b0e255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 150, 128)          1280000   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 150, 128)          131584    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,461,122\n",
      "Trainable params: 1,461,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(maxWordsCount, 128, input_length = max_text_len))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d293a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=Adam(0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0d238f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "model_lstm_save_path = '32_model_lstm.h5' ### для автоматического сохранения лучшей модели\n",
    "checkpoint_callback_lstm = ModelCheckpoint(model_lstm_save_path, \n",
    "                                      monitor='val_accuracy',\n",
    "                                      save_best_only=True,\n",
    "                                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5542b845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.4095 - accuracy: 0.8037\n",
      "Epoch 1: val_accuracy improved from -inf to 0.86590, saving model to 32_model_lstm.h5\n",
      "1250/1250 [==============================] - 358s 283ms/step - loss: 0.4095 - accuracy: 0.8037 - val_loss: 0.3247 - val_accuracy: 0.8659\n",
      "Epoch 2/5\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.2718 - accuracy: 0.8891\n",
      "Epoch 2: val_accuracy did not improve from 0.86590\n",
      "1250/1250 [==============================] - 347s 278ms/step - loss: 0.2718 - accuracy: 0.8891 - val_loss: 0.3290 - val_accuracy: 0.8620\n",
      "Epoch 3/5\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.2338 - accuracy: 0.9076\n",
      "Epoch 3: val_accuracy improved from 0.86590 to 0.86620, saving model to 32_model_lstm.h5\n",
      "1250/1250 [==============================] - 349s 279ms/step - loss: 0.2338 - accuracy: 0.9076 - val_loss: 0.3201 - val_accuracy: 0.8662\n",
      "Epoch 4/5\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.2085 - accuracy: 0.9200\n",
      "Epoch 4: val_accuracy improved from 0.86620 to 0.86680, saving model to 32_model_lstm.h5\n",
      "1250/1250 [==============================] - 345s 276ms/step - loss: 0.2085 - accuracy: 0.9200 - val_loss: 0.3269 - val_accuracy: 0.8668\n",
      "Epoch 5/5\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.1848 - accuracy: 0.9314\n",
      "Epoch 5: val_accuracy did not improve from 0.86680\n",
      "1250/1250 [==============================] - 335s 268ms/step - loss: 0.1848 - accuracy: 0.9314 - val_loss: 0.3908 - val_accuracy: 0.8563\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, Y, batch_size=32, epochs=5,validation_split=0.2,callbacks=[checkpoint_callback_lstm])#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "853b1d23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 52ms/step\n",
      "[[0.7361074  0.26389262]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "### Отметка первого нейрона это положительный отзыв, значит np.argmax с выходом 0 значит, что отзыв положительный\n",
    "t = \"I liked the movie I will watch it again very cool\".lower()\n",
    "data2 = tokenizer.texts_to_sequences([t])\n",
    "\n",
    "data_pad2 = pad_sequences(data2, maxlen=max_text_len)\n",
    "\n",
    "\n",
    "res = model.predict(data_pad2)\n",
    "print(res, np.argmax(res), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7a710439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Положительный: 7/10\n"
     ]
    }
   ],
   "source": [
    "### На основании значения выходного нейрона можно распределять значения оценки по 10-бальной шкале\n",
    "\n",
    "if np.argmax(res)==0:\n",
    "    status='Положительный:'\n",
    "    if 0.6 <res[0][0]< 0.75:\n",
    "        score = '7/10'\n",
    "    elif 0.75 <res[0][0]< 0.9:\n",
    "        score = '8/10'\n",
    "    else:\n",
    "        score = '9/10'\n",
    "else:\n",
    "    status='Отрицательный'\n",
    "    ### по аналогии можно прописать градацию\n",
    "    \n",
    "print(status,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1f82ed07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizer.joblib']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(tokenizer, 'tokenizer.joblib') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
